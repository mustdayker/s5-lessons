# Для комментариев


__ 17.07.2023 __

Максим, привет!

- Параметры подключения в комментах это чисто для моего удобства в учебе, на реальнрой задаче я так делать разумеется не буду

## ИСПРАВЛЕНИЯ

- Исправил сам DAG, поставил на расписание, исправил стартовую дату, сделал заполнение данными со стартовой даты

```python
with DAG(
    "project_s5_v1", 
    start_date=datetime(2023, 7, 10), 
    schedule_interval='@daily',
    catchup=True
    )
```

- В функцию для скачивания записей о доставках передал атрибут ds, после чего в API данные засунул в следующем виде:

```python
    run_day = datetime.strptime(ds, "%Y-%m-%d")
    date_from = run_day - timedelta(days=1)
    date_to = run_day
```


- Добавил инкрементную загрузку при заполнении dds.dm_delivers

```sql
WHERE   delivery_ts >= '{{ds}}'::date - INTERVAL '1 day'
    AND delivery_ts <= '{{ds}}'::date
```

## ВОПРОС
Вроде все отработало, но у меня в STG слое 2016 записей о доставках, а в DDS слое 2001 запись.
Это из-за разницы в настройках часовых поясов в Airflow и Postgres произошло?

Остальные серые комментарии переделывать с твоего разрешения не буду, сил уже нет и по программе отстал. Советы в будущем учту, большое спасибо тебе!


__ 16.07.2023 __

Привет, ревьювер.
Меня зовут, Дмитрий, рад заочно познакомится.
К сожалению этот спринт для меня прошел как в тумане, был сильный скачек сложности по реализации DAG, в итоге было очень интересно, но ничего непонятно.
По итогу DAG я писал по старинке, без использования ООП. К сожалению на текущем уровне я не смогу реализовать данный проект с ООП. 

